\chapter{Introduction}

\section{Why parallel programming?}
Parallel hardware is everywhere. Parallel programming is about expressing
concurrency safely and efficiently.

\begin{ppnote}
A recurring theme: performance is a \emph{tradeoff} between work, critical path,
and overheads (communication, synchronization, scheduling).
\end{ppnote}

\section{A first OpenMP program}

\begin{ppexample}
We begin with a simple data-parallel loop and then reason about correctness and speedup.
\end{ppexample}

\begin{ppprogram}{OpenMP reduction for summation}{prog:omp-sum}
\begin{lstlisting}[language=OpenMP]
#include <omp.h>
#include <stdio.h>

int main() {
  const int n = 1000000;
  double sum = 0.0;

  #pragma omp parallel for reduction(+:sum)
  for (int i = 1; i <= n; i++) sum += 1.0 / i;

  printf("sum = %.12f\n", sum);
  return 0;
}
\end{lstlisting}
\end{ppprogram}

\begin{pppitfall}
Do not assume a reduction fixes all correctness issues: shared data structures,
I/O, and non-associative floating-point operations can still surprise you.
\end{pppitfall}

\section{Exercises}
\begin{ppexercises}
  \ppexercise{Modify Program~\ref{prog:omp-sum} to use \ppinline{schedule(static)}
  and \ppinline{schedule(dynamic)}. Measure runtimes and explain the difference.}

  \ppexercise{Explain why summation order affects floating-point results. Suggest
  a mitigation strategy.}
\end{ppexercises}
